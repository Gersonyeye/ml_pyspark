{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d652df3",
   "metadata": {},
   "source": [
    "# Instalación de Spark en Windows\n",
    "\n",
    "## Instalar Java\n",
    "\n",
    "PySpark requiere Java 17/21.\n",
    "\n",
    "1. Verificar en `cmd` si se tiene instalado Java con el comando `java --version`.\n",
    "2. Si no se tiene instalado, descargar el instalador de la version 17.0.14 de Java (JDK 17) en [https://www.oracle.com/java/technologies/javase/jdk17-0-13-later-archive-downloads.html#license-lightbox](https://www.oracle.com/java/technologies/javase/jdk17-0-13-later-archive-downloads.html#license-lightbox)\n",
    "3. Descargar el instalador `Windows x64 MSI Installer` de la versión 17.0.14.\n",
    "4. Instala Java 17 y guarda, antes de instalar, la ruta donde se va a instalar Java pues nos servirá luego. Debe tener permisos de administrador. Por defecto lo instala en la ruta `C:\\Program Files\\Java\\jdk-17`.\n",
    "\n",
    "\n",
    "## Instalar Python 3.11.9\n",
    "\n",
    "En este caso, para comandar Spark, utilizaremos la API de Spark con Python.\n",
    "\n",
    "1. Verificar en `cmd` si se tiene instalado Python con el comando `python --version`.\n",
    "2. Si no se tiene instalado, descargar el instalador de la version de Python 3.11.9 en [https://www.python.org/ftp/python/3.11.9/python-3.11.9-amd64.exe](https://www.python.org/ftp/python/3.11.9/python-3.11.9-amd64.exe)\n",
    "3. Ejecuta el instalador de Python 3.11.9\n",
    "4. Selecciona la opción `Customize installation`.\n",
    "5. Deben estar seleccionadas todas las casillas de `Optional Features`.\n",
    "6. Si en `Advanced Options` no se tiene seleccionada la opción `Add Python to environment variables`, selecciona esta opción y presiona `Install` (Ten en cuenta que debes tener permisos de administrador).\n",
    "7. Espera a que termine la instalación y da clic en `Close`.\n",
    "\n",
    "## Instalar Spark 3.5.7\n",
    "\n",
    "1. Ve a la página [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html).\n",
    "2. Deja en la primera lista desplegable (Choose a Spark release) seleccionar la versión 3.5.7 del 24 de septiembre del 2025.\n",
    "3. Selecciona en la segunda lista desplegable (Choose a package type) la opción `Pre-built for Apache Hadoop 3.3 and later`.\n",
    "4. Da clic en el enlace que se encuentra en el paso 3 (Download Spark) para comenzar a descargar el archivo comprimido de Spark.\n",
    "5. Se direccionará a otra página, la cual dará diferentes enlaces para realizar la descarga. Se debe seleccionar el primer enlace y la descarga iniciará.\n",
    "6. En la carpeta de descargas, descomprima el archivo de spark (`spark-3.5.7-bin-hadoop3.tgz`) con la opción \"Extraer todo\" en la misma carpeta de descargas.\n",
    "7. En la carpeta de descargas aparecerá una carpeta nombrada con el mismo nombre del archivo comprimido (`spark-3.5.7-bin-hadoop3`), dentro de ella encontrarás una carpeta con el mismo nombre, entra en ella y copia todos los archivos que se encuentran dentro.\n",
    "8. Abre una ventana del Explorador de archivos y entra en la unidad `C:`.\n",
    "9. Crea una carpeta en la unidad `C:` llamada `spark`.\n",
    "10. Pege en la carpeta `spark` todos los archivos copiados anteriormente.\n",
    "\n",
    "\n",
    "## Winutils para Hadoop 3.3\n",
    "\n",
    "Para comandar Hadoop desde Spark en Windows, debemos descargar un archivo ejecutable que nos ayudará con esto.\n",
    "\n",
    "1. Descarga `winutils.exe` en el repositorio [https://github.com/cdarlint/winutils/blob/master/hadoop-3.3.6/bin/winutils.exe](https://github.com/cdarlint/winutils/blob/master/hadoop-3.3.6/bin/winutils.exe).\n",
    "2. Crea una carpeta en la unidad `C:` llamada `hadoop`.\n",
    "3. Dentro de la carpeta `hadoop` crea una carpeta llamada `bin`.\n",
    "4. Dentro de la carpeta `bin`, pega el archivo ejecutable que descargaste llamado `winutils.exe`.\n",
    "\n",
    "\n",
    "## Configurar variables de entorno\n",
    "\n",
    "Se deben configurar las variables de entorno para ejecutar Hadoop, Spark y Java.\n",
    "\n",
    "1. Escriba en el buscador de la barra de tareas \"variables de entorno\" y seleccione la opción \"Editar las variables de entorno del sistema\".\n",
    "2. En la ventana que se abre, dar clic en el botón \"Variables de entorno\".\n",
    "3. En el recuadro de \"Variables del sistema\", dar clic en el botón \"Nueva...\"\n",
    "4. En \"Nombre de la variable\" escribe `JAVA_HOME` y en \"Valor de la variable\" escribe la ruta de instalación de Java `C:\\Program Files\\Java\\jdk-17` o da clic en \"Examinar directorio\" y selecciona la carpeta de instalación de Java. Para finalizar da clic en el botón \"Aceptar\".\n",
    "5. Repite los pasos 3 y 4 con la siguiente información:\n",
    "    - `SPARK_HOME` y `C:\\spark`.\n",
    "    - `HADOOP_HOME` y `C:\\hadoop`.\n",
    "6. Selecciona en el listado de variables del sistema la variable `Path`, luego da clic en el botón \"Editar...\".\n",
    "7. En la ventana que se abre, da clic en el botón \"Nuevo\" y escribe `%JAVA_HOME%\\bin`. Luego presiona \"Enter\".\n",
    "8. Repite el paso 7 pero con la siguiente información:\n",
    "    - `%SPARK_HOME%\\bin`.\n",
    "    - `%HADOOP_HOME%\\bin`.\n",
    "9. Al finalizar de introducir las rutas en la variable \"Path\", da clic en el botón \"Aceptar\".\n",
    "10. En la ventana de variables del sistema da clic en el botón \"Aceptar\".\n",
    "\n",
    "\n",
    "\n",
    "# Verificar instalación de Spark\n",
    "\n",
    "1. Abre una terminal (`cmd`).\n",
    "2. Escribe `pyspark` y presiona \"Enter\".\n",
    "3. Se iniciará la sesión de Spark en la terminal.\n",
    "4. Para salir, escribe `quit()` y presiona \"Enter\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce9b68",
   "metadata": {},
   "source": [
    "# Crear carpeta de proyecto\n",
    "\n",
    "En la ubicación que desee cree una carpeta con el nombre `ml_pyspark`. Ingrese y cree una carpeta llamada `sesion1`\n",
    "\n",
    "# Instalar VSC\n",
    "\n",
    "Diríjase a la url [https://code.visualstudio.com/Download](https://code.visualstudio.com/Download). Descargue y ejecute el instalador para Windows.\n",
    "\n",
    "Luego siga las indicaciones de configuración que dará el profesor.\n",
    "\n",
    "# Crear ambiente de programación\n",
    "\n",
    "En la terminal de VSC, corra la siguiente línea de código: `python -m venv venv`.\n",
    "\n",
    "# Instalar librerías en el ambiente\n",
    "\n",
    "Luego de crearse el ambiente de programación, corra la siguiente línea de código: `venv\\Scripts\\activate`. A continuación copie y pegue en la carpeta del proyecto el archivo requirements.txt que se encontrará en la carpeta del curso.\n",
    "\n",
    "Seguido a esto, corra la siguiente línea de código: `pip install -r requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29817c98",
   "metadata": {},
   "source": [
    "# Crear sesión en jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38c1b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Forzar a Spark a usar el Python de tu venv\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a8335",
   "metadata": {},
   "source": [
    "# Verificar host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0beefcb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Yeye:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f67f70bb90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e13578",
   "metadata": {},
   "source": [
    "# Verificar versión de Python en jupyter y de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44976b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python del notebook: c:\\diplomado\\ml_pyspark\\venv\\Scripts\\python.exe\n",
      "Versión Spark: 3.5.7\n"
     ]
    }
   ],
   "source": [
    "print(\"Python del notebook:\", sys.executable)\n",
    "print(\"Versión Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c017e",
   "metadata": {},
   "source": [
    "# Verificar versión de Java en la sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a67dcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0.14\n",
      "Oracle Corporation\n",
      "C:\\Program Files\\Java\\jdk-17\n"
     ]
    }
   ],
   "source": [
    "print(spark._jvm.java.lang.System.getProperty(\"java.version\"))\n",
    "print(spark._jvm.java.lang.System.getProperty(\"java.vendor\"))\n",
    "print(spark._jvm.java.lang.System.getProperty(\"java.home\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc81ed92",
   "metadata": {},
   "source": [
    "# Verificar creando un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d06330a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   Ana|  25|\n",
      "|  2|  Luis|  30|\n",
      "|  3| Marta|  28|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Ana\", 25),\n",
    "    (2, \"Luis\", 30),\n",
    "    (3, \"Marta\", 28)\n",
    "    ]\n",
    "df = spark.createDataFrame(data, [\"id\", \"nombre\", \"edad\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46a1e4",
   "metadata": {},
   "source": [
    "# Prueba de exportar el dataframe a pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4faaca0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nombre</th>\n",
       "      <th>edad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ana</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Luis</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Marta</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id nombre  edad\n",
       "0   1    Ana    25\n",
       "1   2   Luis    30\n",
       "2   3  Marta    28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67eeb74",
   "metadata": {},
   "source": [
    "# Cerrar la sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51f9ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
